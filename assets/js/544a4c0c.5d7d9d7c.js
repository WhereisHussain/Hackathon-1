"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[8997],{7237(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"chapter-3","title":"Chapter 3: Sensing and Perception","description":"For a humanoid robot, the world is a chaotic stream of data. Perception is the process of turning raw sensor readings into a structured understanding of the environment.","source":"@site/docs/chapter-3.md","sourceDirName":".","slug":"/chapter-3","permalink":"/Hackathon-1/docs/chapter-3","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter-3.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Translate your site","permalink":"/Hackathon-1/docs/tutorial-extras/translate-your-site"},"next":{"title":"Chapter 4: Actuation and Control","permalink":"/Hackathon-1/docs/chapter-4"}}');var o=t(4848),r=t(8453);const s={sidebar_position:4},a="Chapter 3: Sensing and Perception",c={},l=[{value:"Visual Perception",id:"visual-perception",level:2},{value:"Tactile Sensing: The Sense of Touch",id:"tactile-sensing-the-sense-of-touch",level:2},{value:"Proprioception: Internal Sensing",id:"proprioception-internal-sensing",level:2},{value:"AI in Perception",id:"ai-in-perception",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-3-sensing-and-perception",children:"Chapter 3: Sensing and Perception"})}),"\n",(0,o.jsx)(n.p,{children:"For a humanoid robot, the world is a chaotic stream of data. Perception is the process of turning raw sensor readings into a structured understanding of the environment."}),"\n",(0,o.jsx)(n.h2,{id:"visual-perception",children:"Visual Perception"}),"\n",(0,o.jsx)(n.p,{children:"Humanoids rely heavily on vision for navigation and interaction."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Stereo Vision"}),": Using two cameras to calculate depth through triangulation, similar to human eyes."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Detection and Segmentation"}),": Using models like YOLO (You Only Look Once) or Segment Anything (SAM) to identify obstacles and tools."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Visual SLAM (Simultaneous Localization and Mapping)"}),": Building a map of an unknown environment while tracking the robot's location within it."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"tactile-sensing-the-sense-of-touch",children:"Tactile Sensing: The Sense of Touch"}),"\n",(0,o.jsx)(n.p,{children:"Interaction requires more than just sight. A robot needs to know how hard it is gripping an object."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pressure Sensors"}),": Arrays of sensors on the fingertips that detect contact force."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Slip Detection"}),": High-frequency tactile data used to determine if an object is sliding out of the robot's grasp."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"proprioception-internal-sensing",children:"Proprioception: Internal Sensing"}),"\n",(0,o.jsx)(n.p,{children:"Proprioception is the robot's awareness of its own body state."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Joint Encoders"}),": High-precision sensors that measure the exact angle of every motor."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"IMU Integration"}),": Combining accelerometer and gyroscope data to maintain an estimate of the robot's orientation (pitch, roll, yaw)."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"ai-in-perception",children:"AI in Perception"}),"\n",(0,o.jsx)(n.p,{children:'Modern perception systems use "Foundation Models" to generalize across different environments. Instead of programming for specific objects, we use Large Vision Models (LVMs) that can understand semantic concepts like "fragile" or "slippery".'})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>a});var i=t(6540);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);